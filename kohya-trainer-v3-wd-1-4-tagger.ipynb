{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer-v3-wd-1-4-tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kohya Trainer V3 - VRAM 12GB\n",
        "###Best way to train Stable Diffusion model for peeps who didn't have good GPU"
      ],
      "metadata": {
        "id": "slgjeYgd6pWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted to Google Colab based on [Kohya Guide](https://note.com/kohya_ss/n/nbf7ce8d80f29#c9d7ee61-5779-4436-b4e6-9053741c46bb)\n",
        "\n",
        "Adapted to Google Colab by [Linaqruf](https://github.com/Linaqruf)\n",
        "\n",
        "You can find latest notebook update [here](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer-v3-wd-1-4-tagger.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gPgBR3KM6E-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is this?\n"
      ],
      "metadata": {
        "id": "v3Qxv-rCXshE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#####**_Q: So what's differences between `Kohya Trainer` and other trainer out there?_**\n",
        "#####A: **Kohya Trainer** have some new features like\n",
        "1. Using the U-Net learning\n",
        "2. Automatic captioning/tagging for every image automatically with BLIP/DeepDanbooru\n",
        "3. Implemented [NovelAI Aspect Ratio Bucketing Tool](https://github.com/NovelAI/novelai-aspect-ratio-bucketing) so you don't need to crop image dataset 512x512 ever again\n",
        "- Use the output of the second-to-last layer of CLIP (Text Encoder) instead of the last layer.\n",
        "- Learning at non-square resolutions (Aspect Ratio Bucketing) .\n",
        "- Extend token length from 75 to 225.\n",
        "4. By preparing a certain number of images (several hundred or more seems to be desirable), you can make learning even more flexible than with DreamBooth.\n",
        "5. It also support Hypernetwork learning\n",
        "6. `NEW!` 23/11 - Implemented Waifu Diffusion 1.4 Tagger for alternative DeepDanbooru to auto-tagging.\n",
        "\n",
        "#####**_Q: And what's differences between this notebook and other dreambooth notebook out there?_**\n",
        "#####A: We're adding Quality of Life features such as:\n",
        "- Added pruning option\n",
        "- Install **gallery-dl** to scrap images, so you can get your own dataset fast with google bandwidth\n",
        "- Huggingface Integration, here you can login to huggingface-hub and upload your trained model/dataset to huggingface\n",
        "---"
      ],
      "metadata": {
        "id": "gSSojWxg7cFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Dependencies"
      ],
      "metadata": {
        "id": "h3AuTNu6MFZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Diffuser\n",
        "%cd /content/\n",
        "!pip install --upgrade pip\n",
        "!pip install diffusers[torch]==0.7.2"
      ],
      "metadata": {
        "id": "Aq5cjtG5nJ3Y",
        "cellView": "form",
        "outputId": "f2604f92-6243-4188-cdde-f4f05f6a2f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 32.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffusers[torch]==0.7.2\n",
            "  Downloading diffusers-0.7.2-py3-none-any.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.9/304.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (3.8.0)\n",
            "Requirement already satisfied: Pillow<10.0 in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (7.1.2)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (2022.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (1.21.6)\n",
            "Collecting accelerate>=0.11.0\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from diffusers[torch]==0.7.2) (1.12.1+cu113)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate>=0.11.0->diffusers[torch]==0.7.2) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from accelerate>=0.11.0->diffusers[torch]==0.7.2) (21.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate>=0.11.0->diffusers[torch]==0.7.2) (5.4.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.10.0->diffusers[torch]==0.7.2) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.10.0->diffusers[torch]==0.7.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->diffusers[torch]==0.7.2) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers[torch]==0.7.2) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers[torch]==0.7.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers[torch]==0.7.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers[torch]==0.7.2) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->accelerate>=0.11.0->diffusers[torch]==0.7.2) (3.0.9)\n",
            "Installing collected packages: huggingface-hub, accelerate, diffusers\n",
            "Successfully installed accelerate-0.14.0 diffusers-0.7.2 huggingface-hub-0.11.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install xformers\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "from IPython.display import HTML\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "s = getoutput('nvidia-smi')\n",
        "\n",
        "if 'T4' in s:\n",
        "  gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "  gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "  gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "  gpu = 'A100'\n",
        "\n",
        "if (gpu=='T4'):\n",
        "  %pip install -qq https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.14/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "elif (gpu=='P100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif (gpu=='V100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "elif (gpu=='A100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl"
      ],
      "metadata": {
        "id": "Q_DPyXcDqv8J",
        "cellView": "form",
        "outputId": "a97d62bd-ba11-4fbe-9c31-1ed46c0e2a8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Kohya Trainer v3"
      ],
      "metadata": {
        "id": "tTVqCAgSmie4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_u3q60di584x",
        "cellView": "form",
        "outputId": "3bf0abfa-4093-43f6-8aeb-029c810cb548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'kohya-trainer'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 195 (delta 24), reused 4 (delta 4), pack-reused 156\u001b[K\n",
            "Receiving objects: 100% (195/195), 88.77 KiB | 17.75 MiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Cloning Kohya Trainer v3\n",
        "%cd /content/\n",
        "!git clone https://github.com/Linaqruf/kohya-trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Kohya Trainer v3 Requirement\n",
        "%cd /content/kohya-trainer\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "WNn0g1pnHfk5",
        "cellView": "form",
        "outputId": "15c3b844-dcca-4aa2-d036-be473384c155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.12.1+cu113)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.14.0)\n",
            "Collecting transformers>=4.21.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.3.post0-py3-none-any.whl (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.35.4-py3-none-any.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 2)) (5.4.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.21.0->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.21.0->-r requirements.txt (line 3)) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.21.0->-r requirements.txt (line 3)) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.21.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.21.0->-r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.21.0->-r requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->-r requirements.txt (line 4)) (0.2.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r requirements.txt (line 5)) (4.6.0.66)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations->-r requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r requirements.txt (line 5)) (0.18.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations->-r requirements.txt (line 5)) (0.0.4)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning->-r requirements.txt (line 8)) (2022.11.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (3.8.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->accelerate->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations->-r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.2->pytorch_lightning->-r requirements.txt (line 8)) (3.19.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.21.0->-r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.21.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.21.0->-r requirements.txt (line 3)) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (0.13.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning->-r requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning->-r requirements.txt (line 8)) (2.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=9bd7f5c25a2c8b62ad31c2fb403d9ab0397cf1368198bc590efe3e7a923bf07d\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/21/65/2ac62db55efa6e6edfad09f4e315aa82a35ab138f51e784fb1\n",
            "Successfully built fire\n",
            "Installing collected packages: tokenizers, bitsandbytes, tensorboardX, ftfy, fire, einops, torchmetrics, lightning-utilities, transformers, pytorch_lightning\n",
            "Successfully installed bitsandbytes-0.35.4 einops-0.6.0 fire-0.4.0 ftfy-6.1.1 lightning-utilities-0.3.0 pytorch_lightning-1.8.3.post0 tensorboardX-2.5.1 tokenizers-0.13.2 torchmetrics-0.10.3 transformers-4.24.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Collecting datasets\n",
        "You can either upload your datasets to this notebook or use image scraper below to bulk download images from danbooru.\n",
        "\n",
        "If you want to use your own datasets, make sure to put them in a folder titled `train_data` in `content/kohya-trainer`\n",
        "\n",
        "This is to make the training process easier because the folder that will be used for training is in `content/kohya-trainer/train-data`."
      ],
      "metadata": {
        "id": "En9UUwGNMRMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install `gallery-dl` library\n",
        "!pip install -U gallery-dl"
      ],
      "metadata": {
        "id": "dBi4pk7hy-Jg",
        "cellView": "form",
        "outputId": "2530956c-84d8-4ec7-8d5c-aecaac98bd0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gallery-dl\n",
            "  Downloading gallery_dl-1.24.0-py3-none-any.whl (555 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.0/556.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.11.0 in /usr/local/lib/python3.7/dist-packages (from gallery-dl) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.11.0->gallery-dl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.11.0->gallery-dl) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.11.0->gallery-dl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.11.0->gallery-dl) (3.0.4)\n",
            "Installing collected packages: gallery-dl\n",
            "Successfully installed gallery-dl-1.24.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Danbooru Scraper\n",
        "#@markdown **How this work?**\n",
        "\n",
        "#@markdown By using **gallery-dl** we can scrap or bulk download images on Internet, on this notebook we will scrap images from Danbooru using tag1 and tag2 as target scraping.\n",
        "%cd /content/kohya-trainer\n",
        "\n",
        "tag = \"namazu_(yamasonson)\" #@param {type: \"string\"}\n",
        "tag2 = \"\" #@param {type: \"string\"}\n",
        "output_dir = \"/content/kohya-trainer/train_data\" \n",
        "\n",
        "if tag2 is not \"\":\n",
        "  tag = tag + \"+\" + tag2\n",
        "else:\n",
        "  tag = tag\n",
        "\n",
        "def danbooru_dl():\n",
        "   !gallery-dl \"https://danbooru.donmai.us/posts?tags={tag}+&z=5\" -D {output_dir}\n",
        "\n",
        "danbooru_dl()\n",
        "\n",
        "#@markdown The output directory will be on /content/kohya-trainer/train_data. We also will use this folder as target folder for training next step.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kt1GzntK_apb",
        "cellView": "form",
        "outputId": "870f9fbe-d8c0-498a-fd6f-0d84fe222e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5847330_32c96f737cefc8f6c27ce8adab6190d2.jp…/content/kohya-trainer/train_data/danbooru_5847330_32c96f737cefc8f6c27ce8adab6190d2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5842306_433e32300cee18bbf40387b62a784925.jp…/content/kohya-trainer/train_data/danbooru_5842306_433e32300cee18bbf40387b62a784925.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5836091_dc08a41de605a102a04bf55db9c5b709.jp…/content/kohya-trainer/train_data/danbooru_5836091_dc08a41de605a102a04bf55db9c5b709.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5836081_9e1ed257f4ef47307bae397fd8b5ba0e.jp…/content/kohya-trainer/train_data/danbooru_5836081_9e1ed257f4ef47307bae397fd8b5ba0e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5827909_e44002fc37c36c02f0053a83ac829321.jp…/content/kohya-trainer/train_data/danbooru_5827909_e44002fc37c36c02f0053a83ac829321.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5823391_27db6d2ff8837c4a7f27eb8c4f969207.jp…/content/kohya-trainer/train_data/danbooru_5823391_27db6d2ff8837c4a7f27eb8c4f969207.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5823382_a16a94a1113957b7cbb4edd92f95e684.jp…/content/kohya-trainer/train_data/danbooru_5823382_a16a94a1113957b7cbb4edd92f95e684.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5812653_5acaffb943da54bfab8480907aa4fa7c.jp…/content/kohya-trainer/train_data/danbooru_5812653_5acaffb943da54bfab8480907aa4fa7c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5807722_1efcd14345f2b6d546349765a267a4f8.jp…/content/kohya-trainer/train_data/danbooru_5807722_1efcd14345f2b6d546349765a267a4f8.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5801719_bc8b49d2a481cef80411bb9c07560e0a.jp…/content/kohya-trainer/train_data/danbooru_5801719_bc8b49d2a481cef80411bb9c07560e0a.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5797624_3d06172451e01c2a7fa84c43a89d9099.jp…/content/kohya-trainer/train_data/danbooru_5797624_3d06172451e01c2a7fa84c43a89d9099.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5790700_859a7860d9e1be47d31161c37a497035.jp…/content/kohya-trainer/train_data/danbooru_5790700_859a7860d9e1be47d31161c37a497035.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5783739_da9213a46762cdd1b10f77311fd26357.jp…/content/kohya-trainer/train_data/danbooru_5783739_da9213a46762cdd1b10f77311fd26357.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5777601_5318fed257dda9d86a097756b1c2aa94.jp…/content/kohya-trainer/train_data/danbooru_5777601_5318fed257dda9d86a097756b1c2aa94.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5770497_6218b01f0e0839b4a666b5ce3c299042.jp…/content/kohya-trainer/train_data/danbooru_5770497_6218b01f0e0839b4a666b5ce3c299042.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5765154_22692e99d70dbb4f4ade09c4a05f56ac.jp…/content/kohya-trainer/train_data/danbooru_5765154_22692e99d70dbb4f4ade09c4a05f56ac.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5753683_ca05a2796ccf9214c55fc82720e6702f.jp…/content/kohya-trainer/train_data/danbooru_5753683_ca05a2796ccf9214c55fc82720e6702f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5748249_7aee91d9dc906a27f189ffd5c741dd47.jp…/content/kohya-trainer/train_data/danbooru_5748249_7aee91d9dc906a27f189ffd5c741dd47.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5743344_f3f09a79522e32cc3feb343909f24d05.jp…/content/kohya-trainer/train_data/danbooru_5743344_f3f09a79522e32cc3feb343909f24d05.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5733577_f82a9f2f580f0038cb5cb53c445ad26f.jp…/content/kohya-trainer/train_data/danbooru_5733577_f82a9f2f580f0038cb5cb53c445ad26f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5725158_39e687f4f14c8a5630cb2caa29b5c6cc.jp…/content/kohya-trainer/train_data/danbooru_5725158_39e687f4f14c8a5630cb2caa29b5c6cc.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5716517_cb0cd6f5c2f25352c3fa6e35c71a926e.jp…/content/kohya-trainer/train_data/danbooru_5716517_cb0cd6f5c2f25352c3fa6e35c71a926e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5709555_7c50e22bdb711e4d1c70bdaa6acda0fe.jp…/content/kohya-trainer/train_data/danbooru_5709555_7c50e22bdb711e4d1c70bdaa6acda0fe.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5704672_cec7cf70d6d0adbcab7d70deec3073b8.jp…/content/kohya-trainer/train_data/danbooru_5704672_cec7cf70d6d0adbcab7d70deec3073b8.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5698444_ba217e0654c8f0d07a97a7bdc8e76d0a.jp…/content/kohya-trainer/train_data/danbooru_5698444_ba217e0654c8f0d07a97a7bdc8e76d0a.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5691471_3962cd38a47561884594d0554239a1d7.jp…/content/kohya-trainer/train_data/danbooru_5691471_3962cd38a47561884594d0554239a1d7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5686342_1abe7eec1fe4b760934e73f7efe8e043.jp…/content/kohya-trainer/train_data/danbooru_5686342_1abe7eec1fe4b760934e73f7efe8e043.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5679969_572b93036667cef000e00d28fb620504.jp…/content/kohya-trainer/train_data/danbooru_5679969_572b93036667cef000e00d28fb620504.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5673080_3edd644c0bdeb854af088fe00720a53c.jp…/content/kohya-trainer/train_data/danbooru_5673080_3edd644c0bdeb854af088fe00720a53c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5668109_6da51a1a7f5fe4994d43a91919f3ad37.jp…/content/kohya-trainer/train_data/danbooru_5668109_6da51a1a7f5fe4994d43a91919f3ad37.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5664112_1b28696bbd2d05b80f0572b0b6d64a2f.jp…/content/kohya-trainer/train_data/danbooru_5664112_1b28696bbd2d05b80f0572b0b6d64a2f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5661998_8d4d241e143e2a995ed8ac1feaf039d9.jp…/content/kohya-trainer/train_data/danbooru_5661998_8d4d241e143e2a995ed8ac1feaf039d9.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5661987_492d432f7df33c3d1c2862d365665721.jp…/content/kohya-trainer/train_data/danbooru_5661987_492d432f7df33c3d1c2862d365665721.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5655266_be88bb16a582fc92bb6a6d5cd0168166.jp…/content/kohya-trainer/train_data/danbooru_5655266_be88bb16a582fc92bb6a6d5cd0168166.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5650648_4975262155aa2d6a1c1b99f3ad40bed2.jp…/content/kohya-trainer/train_data/danbooru_5650648_4975262155aa2d6a1c1b99f3ad40bed2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5644399_ae1fc4f40fdad65fe3f2acaa8ff8d277.jp…/content/kohya-trainer/train_data/danbooru_5644399_ae1fc4f40fdad65fe3f2acaa8ff8d277.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5637621_033f3cd4a14b8ede595cec15a6e1475d.jp…/content/kohya-trainer/train_data/danbooru_5637621_033f3cd4a14b8ede595cec15a6e1475d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5629886_2e2b89f5d371ed758b509245e19b394d.jp…/content/kohya-trainer/train_data/danbooru_5629886_2e2b89f5d371ed758b509245e19b394d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5619940_a42f2a3012b0a192937fa5599b220957.jp…/content/kohya-trainer/train_data/danbooru_5619940_a42f2a3012b0a192937fa5599b220957.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5612710_e8c674898444af9ee06ab1bc9e7ca876.jp…/content/kohya-trainer/train_data/danbooru_5612710_e8c674898444af9ee06ab1bc9e7ca876.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5610446_ce31bf646d94cdb71825c08a138a77ad.jp…/content/kohya-trainer/train_data/danbooru_5610446_ce31bf646d94cdb71825c08a138a77ad.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5601032_5407724d8087fd0ee9a49d4fcc15d6db.jp…/content/kohya-trainer/train_data/danbooru_5601032_5407724d8087fd0ee9a49d4fcc15d6db.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5592315_c4243ba8b59902d40efe6e82e81f5b8c.jp…/content/kohya-trainer/train_data/danbooru_5592315_c4243ba8b59902d40efe6e82e81f5b8c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5586287_4b9a5b4fc3c56a5c073addc9aa20bd82.jp…/content/kohya-trainer/train_data/danbooru_5586287_4b9a5b4fc3c56a5c073addc9aa20bd82.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5575998_00b1330be9d53aa7e49edbb4ad0715f3.jp…/content/kohya-trainer/train_data/danbooru_5575998_00b1330be9d53aa7e49edbb4ad0715f3.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5569587_4c10193ec5532b6c5737131146b3c1eb.jp…/content/kohya-trainer/train_data/danbooru_5569587_4c10193ec5532b6c5737131146b3c1eb.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5569574_5f6821ce19de76da37ab50bd22a5cd5d.jp…/content/kohya-trainer/train_data/danbooru_5569574_5f6821ce19de76da37ab50bd22a5cd5d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5562655_6bcf3017bf7c625e3ee2dd8ad5b2beeb.jp…/content/kohya-trainer/train_data/danbooru_5562655_6bcf3017bf7c625e3ee2dd8ad5b2beeb.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5557353_3ee2b7bfc67b4618a51c225310ba0719.jp…/content/kohya-trainer/train_data/danbooru_5557353_3ee2b7bfc67b4618a51c225310ba0719.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5550904_b19ff6c0e2659f3c89b5159c8f01f332.jp…/content/kohya-trainer/train_data/danbooru_5550904_b19ff6c0e2659f3c89b5159c8f01f332.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5544047_660d2a0ad324ecbb588928780d884b6a.jp…/content/kohya-trainer/train_data/danbooru_5544047_660d2a0ad324ecbb588928780d884b6a.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5538674_14e876286556954bc7e674c5bd44331c.jp…/content/kohya-trainer/train_data/danbooru_5538674_14e876286556954bc7e674c5bd44331c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5533451_907634786f2c02005932686ebf084477.jp…/content/kohya-trainer/train_data/danbooru_5533451_907634786f2c02005932686ebf084477.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5528175_04409b3d977286b031ad93742f237da3.jp…/content/kohya-trainer/train_data/danbooru_5528175_04409b3d977286b031ad93742f237da3.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5520593_34c272d26bb054d445e1a2b3b0687a17.jp…/content/kohya-trainer/train_data/danbooru_5520593_34c272d26bb054d445e1a2b3b0687a17.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5514441_501a5cc3db75b5771923c0e3906b6939.jp…/content/kohya-trainer/train_data/danbooru_5514441_501a5cc3db75b5771923c0e3906b6939.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5507827_b47e14a7c7bd647b8ce30ae57874db3c.jp…/content/kohya-trainer/train_data/danbooru_5507827_b47e14a7c7bd647b8ce30ae57874db3c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5502835_1e22e08481992279cb9a0b5434c72ca7.jp…/content/kohya-trainer/train_data/danbooru_5502835_1e22e08481992279cb9a0b5434c72ca7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5495978_f86e3d22ec08a77c2430210a12183c80.jp…/content/kohya-trainer/train_data/danbooru_5495978_f86e3d22ec08a77c2430210a12183c80.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5489544_307afd80f9a989823dc44e5929f4b918.jp…/content/kohya-trainer/train_data/danbooru_5489544_307afd80f9a989823dc44e5929f4b918.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5484330_b41319d80cf3e4010c5e8a82b37f4b2e.jp…/content/kohya-trainer/train_data/danbooru_5484330_b41319d80cf3e4010c5e8a82b37f4b2e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5478015_d544c614b52b504bec378eeb0103e4a9.jp…/content/kohya-trainer/train_data/danbooru_5478015_d544c614b52b504bec378eeb0103e4a9.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5471382_569ea6776ff69b0b46507513cf5685b2.jp…/content/kohya-trainer/train_data/danbooru_5471382_569ea6776ff69b0b46507513cf5685b2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5466409_56c119dc90927050f81977152ecbb38b.jp…/content/kohya-trainer/train_data/danbooru_5466409_56c119dc90927050f81977152ecbb38b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5460801_44cb22e18d02c40314022989b7857dc5.jp…/content/kohya-trainer/train_data/danbooru_5460801_44cb22e18d02c40314022989b7857dc5.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5460800_1e650c010c9c49baa81e403b84a49d6d.jp…/content/kohya-trainer/train_data/danbooru_5460800_1e650c010c9c49baa81e403b84a49d6d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5454268_ea3c79e9502fc7ba22cc8efa30270952.jp…/content/kohya-trainer/train_data/danbooru_5454268_ea3c79e9502fc7ba22cc8efa30270952.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5446716_274204ab6d744248c37cd0a70319fdb2.jp…/content/kohya-trainer/train_data/danbooru_5446716_274204ab6d744248c37cd0a70319fdb2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5446713_401bd36f0c24a6db5152478639445f76.jp…/content/kohya-trainer/train_data/danbooru_5446713_401bd36f0c24a6db5152478639445f76.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5446712_f0bf84b2824f10ee88a36feec748664c.jp…/content/kohya-trainer/train_data/danbooru_5446712_f0bf84b2824f10ee88a36feec748664c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5446581_0b2227d082a413ea112ae90e59c744cf.jp…/content/kohya-trainer/train_data/danbooru_5446581_0b2227d082a413ea112ae90e59c744cf.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5438886_363dc868800e0828387ffa449bdb3a49.jp…/content/kohya-trainer/train_data/danbooru_5438886_363dc868800e0828387ffa449bdb3a49.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5431785_d5a7d4ceb2762a71b5c7124b45132e98.jp…/content/kohya-trainer/train_data/danbooru_5431785_d5a7d4ceb2762a71b5c7124b45132e98.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5425652_9298945f63ed84522e5fca05d5c7c3c2.jp…/content/kohya-trainer/train_data/danbooru_5425652_9298945f63ed84522e5fca05d5c7c3c2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5419114_e56251e4380e09a3ef8680ebf685e837.jp…/content/kohya-trainer/train_data/danbooru_5419114_e56251e4380e09a3ef8680ebf685e837.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5413840_ebd860d45e75d02cf0b1e4042860df56.jp…/content/kohya-trainer/train_data/danbooru_5413840_ebd860d45e75d02cf0b1e4042860df56.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5407050_00568f053fb4a3e0aa475465ec6195a3.jp…/content/kohya-trainer/train_data/danbooru_5407050_00568f053fb4a3e0aa475465ec6195a3.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5399905_a60802829facf0a3e0acd88aa3067584.jp…/content/kohya-trainer/train_data/danbooru_5399905_a60802829facf0a3e0acd88aa3067584.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5389257_edcfcac5815b6d0ea284222e53aa85df.jp…/content/kohya-trainer/train_data/danbooru_5389257_edcfcac5815b6d0ea284222e53aa85df.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5382411_307a99543df32b1cc9931be520f037f9.jp…/content/kohya-trainer/train_data/danbooru_5382411_307a99543df32b1cc9931be520f037f9.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5377522_72b8c311acf7c5eb3480ef02b17769a3.jp…/content/kohya-trainer/train_data/danbooru_5377522_72b8c311acf7c5eb3480ef02b17769a3.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5371310_4c8e99e556939c1ef488a4e90da09298.jp…/content/kohya-trainer/train_data/danbooru_5371310_4c8e99e556939c1ef488a4e90da09298.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5364782_1c3f4612585df7da0d7c2396d35a817e.jp…/content/kohya-trainer/train_data/danbooru_5364782_1c3f4612585df7da0d7c2396d35a817e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5359840_7450734c8cc3d4481f75992218ea6640.jp…/content/kohya-trainer/train_data/danbooru_5359840_7450734c8cc3d4481f75992218ea6640.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5354435_9fcef4f5c4f1d3ad748b86faa4ee9723.jp…/content/kohya-trainer/train_data/danbooru_5354435_9fcef4f5c4f1d3ad748b86faa4ee9723.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5345905_361cedda79efee4e6dd419470ed497d0.jp…/content/kohya-trainer/train_data/danbooru_5345905_361cedda79efee4e6dd419470ed497d0.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5340795_b2b070ebde3cada15dd7ce32b9e2a55d.jp…/content/kohya-trainer/train_data/danbooru_5340795_b2b070ebde3cada15dd7ce32b9e2a55d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5335428_0aab5e23359dc5d63348d7b1747b843f.jp…/content/kohya-trainer/train_data/danbooru_5335428_0aab5e23359dc5d63348d7b1747b843f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5333344_0e7753dce16bb44f0592d422e3a1a011.jp…/content/kohya-trainer/train_data/danbooru_5333344_0e7753dce16bb44f0592d422e3a1a011.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5330291_4c13e8ef7342e9ffb139e0e35bb9f4ba.jp…/content/kohya-trainer/train_data/danbooru_5330291_4c13e8ef7342e9ffb139e0e35bb9f4ba.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5324986_5ef43ae69ee854bcf40f6aeaed693c37.jp…/content/kohya-trainer/train_data/danbooru_5324986_5ef43ae69ee854bcf40f6aeaed693c37.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5319905_3a401d5927e772159137911304f7a57e.jp…/content/kohya-trainer/train_data/danbooru_5319905_3a401d5927e772159137911304f7a57e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5310376_43001a842241e609996d809cc8aea65a.jp…/content/kohya-trainer/train_data/danbooru_5310376_43001a842241e609996d809cc8aea65a.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5304614_261e341f3a86f3d3bc4a8c5582519ba1.jp…/content/kohya-trainer/train_data/danbooru_5304614_261e341f3a86f3d3bc4a8c5582519ba1.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5299126_7999edc20964e5bf2ee0d650d76a11b4.jp…/content/kohya-trainer/train_data/danbooru_5299126_7999edc20964e5bf2ee0d650d76a11b4.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5292903_8530b52925e70c31350d2e66a632bc0f.jp…/content/kohya-trainer/train_data/danbooru_5292903_8530b52925e70c31350d2e66a632bc0f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5285569_48859d234dba17c6feb2d58166c6062b.jp…/content/kohya-trainer/train_data/danbooru_5285569_48859d234dba17c6feb2d58166c6062b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5280892_a5717d667bc729543c69ee738f1c587b.jp…/content/kohya-trainer/train_data/danbooru_5280892_a5717d667bc729543c69ee738f1c587b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5270946_be92c359f641b4485f59b0ab4a86bd4b.jp…/content/kohya-trainer/train_data/danbooru_5270946_be92c359f641b4485f59b0ab4a86bd4b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5266487_c8aae7109fd04306ec91331c0a309306.jp…/content/kohya-trainer/train_data/danbooru_5266487_c8aae7109fd04306ec91331c0a309306.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5256467_5de511919b21a0cb4e5c31bbdec801a2.jp…/content/kohya-trainer/train_data/danbooru_5256467_5de511919b21a0cb4e5c31bbdec801a2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5251824_c2a87f9f24d38bf2d433be8c825324da.jp…/content/kohya-trainer/train_data/danbooru_5251824_c2a87f9f24d38bf2d433be8c825324da.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5246765_bbd351745639eb251b564a3d3371252f.jp…/content/kohya-trainer/train_data/danbooru_5246765_bbd351745639eb251b564a3d3371252f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5236321_3074f09985d4f320d5f5f282e1e8e029.jp…/content/kohya-trainer/train_data/danbooru_5236321_3074f09985d4f320d5f5f282e1e8e029.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5231444_5dfbab4da02a1ce50e25c33f45f3a2d5.jp…/content/kohya-trainer/train_data/danbooru_5231444_5dfbab4da02a1ce50e25c33f45f3a2d5.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5220816_551524c09480795786a0561fc236803d.jp…/content/kohya-trainer/train_data/danbooru_5220816_551524c09480795786a0561fc236803d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5213748_3b9b0b9c0621da6451350a837f9bd3e7.jp…/content/kohya-trainer/train_data/danbooru_5213748_3b9b0b9c0621da6451350a837f9bd3e7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5208586_912bf4b63084c315d276edf8a5ca5344.jp…/content/kohya-trainer/train_data/danbooru_5208586_912bf4b63084c315d276edf8a5ca5344.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5201663_97b752a3b07cdbc6fc96518e3d57ef8a.jp…/content/kohya-trainer/train_data/danbooru_5201663_97b752a3b07cdbc6fc96518e3d57ef8a.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5197181_28f57d072fc4c4c198e8871cbfad1060.jp…/content/kohya-trainer/train_data/danbooru_5197181_28f57d072fc4c4c198e8871cbfad1060.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5191391_0948b43a90c40f07ca9beeec2d302d4b.jp…/content/kohya-trainer/train_data/danbooru_5191391_0948b43a90c40f07ca9beeec2d302d4b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5185525_be6ac50faad191ea2516196c7919ef90.jp…/content/kohya-trainer/train_data/danbooru_5185525_be6ac50faad191ea2516196c7919ef90.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5181286_8aab2d27d7f0ec445abdaa9c5a0973c5.jp…/content/kohya-trainer/train_data/danbooru_5181286_8aab2d27d7f0ec445abdaa9c5a0973c5.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5175950_69bd55a3c5f3e928f34cbae2da628639.jp…/content/kohya-trainer/train_data/danbooru_5175950_69bd55a3c5f3e928f34cbae2da628639.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5168779_f420353ece2e30fb80d96b9099da4aa7.jp…/content/kohya-trainer/train_data/danbooru_5168779_f420353ece2e30fb80d96b9099da4aa7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5161710_e5ee8bd81707acc9e69b0d88d94341a6.jp…/content/kohya-trainer/train_data/danbooru_5161710_e5ee8bd81707acc9e69b0d88d94341a6.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5152756_80265ea81433def264f2edc63f3803ab.jp…/content/kohya-trainer/train_data/danbooru_5152756_80265ea81433def264f2edc63f3803ab.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5148288_fd352634ff97cf4cbe630ede1e7d5e91.jp…/content/kohya-trainer/train_data/danbooru_5148288_fd352634ff97cf4cbe630ede1e7d5e91.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5142795_152e6038a59925730158bf0d45a5f96e.jp…/content/kohya-trainer/train_data/danbooru_5142795_152e6038a59925730158bf0d45a5f96e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5136260_3194966ae45c29b1c960651c977672b5.jp…/content/kohya-trainer/train_data/danbooru_5136260_3194966ae45c29b1c960651c977672b5.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5133068_3b8f5270279e4a91bd4d24935874ad98.jp…/content/kohya-trainer/train_data/danbooru_5133068_3b8f5270279e4a91bd4d24935874ad98.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5133067_52e2c9c09db7743fc991437fca0d3ac9.jp…/content/kohya-trainer/train_data/danbooru_5133067_52e2c9c09db7743fc991437fca0d3ac9.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5133066_be0c9c4e0c017ddd393d492593fb0417.jp…/content/kohya-trainer/train_data/danbooru_5133066_be0c9c4e0c017ddd393d492593fb0417.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5132063_146aae59b6b30e0329e410b445cd9a89.jp…/content/kohya-trainer/train_data/danbooru_5132063_146aae59b6b30e0329e410b445cd9a89.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5130081_266fc285280066e04d5a00421e96d1f4.jp…/content/kohya-trainer/train_data/danbooru_5130081_266fc285280066e04d5a00421e96d1f4.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5130074_ddb422ae42d3c1b8dc482d6eb1f88e60.jp…/content/kohya-trainer/train_data/danbooru_5130074_ddb422ae42d3c1b8dc482d6eb1f88e60.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5130069_5af47585b3e04b9e4a342c17f4f75502.jp…/content/kohya-trainer/train_data/danbooru_5130069_5af47585b3e04b9e4a342c17f4f75502.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5125632_26977f88d0b48cdf9936a3ecca355aef.jp…/content/kohya-trainer/train_data/danbooru_5125632_26977f88d0b48cdf9936a3ecca355aef.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5120155_694c9724696596f1fbd3c995d1b0afdf.jp…/content/kohya-trainer/train_data/danbooru_5120155_694c9724696596f1fbd3c995d1b0afdf.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5113371_040c53b4a02e9e8ec7d1716ed8474e08.jp…/content/kohya-trainer/train_data/danbooru_5113371_040c53b4a02e9e8ec7d1716ed8474e08.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5108768_d0b76f1d207e27ce013548d11e2c63db.jp…/content/kohya-trainer/train_data/danbooru_5108768_d0b76f1d207e27ce013548d11e2c63db.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5108676_09d027fe531527ee7894cbf133f9a173.jp…/content/kohya-trainer/train_data/danbooru_5108676_09d027fe531527ee7894cbf133f9a173.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5106570_0865f0ca821096284f69bd0aa6629e4d.jp…/content/kohya-trainer/train_data/danbooru_5106570_0865f0ca821096284f69bd0aa6629e4d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5101360_4d9dd9b7791b6fcacd3964d8826217c7.jp…/content/kohya-trainer/train_data/danbooru_5101360_4d9dd9b7791b6fcacd3964d8826217c7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5089595_19c37197803a67f6f8fc96939d2e6621.jp…/content/kohya-trainer/train_data/danbooru_5089595_19c37197803a67f6f8fc96939d2e6621.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5085025_b33f1f68451857384b0641eae6b303a4.jp…/content/kohya-trainer/train_data/danbooru_5085025_b33f1f68451857384b0641eae6b303a4.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5079916_dabfbb23e496317aeaf3ef7b5069ce6b.jp…/content/kohya-trainer/train_data/danbooru_5079916_dabfbb23e496317aeaf3ef7b5069ce6b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5075311_4438090d55ecbf16026812ae04dd49d8.jp…/content/kohya-trainer/train_data/danbooru_5075311_4438090d55ecbf16026812ae04dd49d8.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5068494_3fe472bdbc8bde9d19decd3c2f445cc8.jp…/content/kohya-trainer/train_data/danbooru_5068494_3fe472bdbc8bde9d19decd3c2f445cc8.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5065102_e0e9f5543840c002f9316ccd17c48514.jp…/content/kohya-trainer/train_data/danbooru_5065102_e0e9f5543840c002f9316ccd17c48514.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5057836_dac6104990fff75ce54bfd7f481eef5d.jp…/content/kohya-trainer/train_data/danbooru_5057836_dac6104990fff75ce54bfd7f481eef5d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5051602_0a006e8f32ace5f2f42b5bddc3496c6c.jp…/content/kohya-trainer/train_data/danbooru_5051602_0a006e8f32ace5f2f42b5bddc3496c6c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5036755_5d71e1f3e29d0693572c3ec9d12200af.jp…/content/kohya-trainer/train_data/danbooru_5036755_5d71e1f3e29d0693572c3ec9d12200af.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5036643_90e4158cb7e0b0ef4f672a14a2fae01f.jp…/content/kohya-trainer/train_data/danbooru_5036643_90e4158cb7e0b0ef4f672a14a2fae01f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5031941_db565dbb4a83c933bfaa15d43b848014.jp…/content/kohya-trainer/train_data/danbooru_5031941_db565dbb4a83c933bfaa15d43b848014.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5016549_bdb5b35bd25fbadc3523c3f4dbd33b6f.jp…/content/kohya-trainer/train_data/danbooru_5016549_bdb5b35bd25fbadc3523c3f4dbd33b6f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5011375_ee8fa524311500858d6f6857c315b9c1.jp…/content/kohya-trainer/train_data/danbooru_5011375_ee8fa524311500858d6f6857c315b9c1.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_5006394_c45fd677ec7d9a7a0d53a99f6b7c99d1.jp…/content/kohya-trainer/train_data/danbooru_5006394_c45fd677ec7d9a7a0d53a99f6b7c99d1.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4983120_bcda94bac92c3facc5362605e2831917.jp…/content/kohya-trainer/train_data/danbooru_4983120_bcda94bac92c3facc5362605e2831917.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4982800_1007c961deedf91fd87457ec48188bf3.jp…/content/kohya-trainer/train_data/danbooru_4982800_1007c961deedf91fd87457ec48188bf3.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4979377_a1546d5915e868dd43273edf4c59b4cf.jp…/content/kohya-trainer/train_data/danbooru_4979377_a1546d5915e868dd43273edf4c59b4cf.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4963848_15653303a1bef9c7111a265623c0548e.jp…/content/kohya-trainer/train_data/danbooru_4963848_15653303a1bef9c7111a265623c0548e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4963808_c7e3946bc70af2b5b352405c05e29af1.jp…/content/kohya-trainer/train_data/danbooru_4963808_c7e3946bc70af2b5b352405c05e29af1.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4961713_0955ff5184ebba05b8d41d4da5d5aa5f.jp…/content/kohya-trainer/train_data/danbooru_4961713_0955ff5184ebba05b8d41d4da5d5aa5f.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4937883_a32eb44246273a47eccac3665e2fddba.jp…/content/kohya-trainer/train_data/danbooru_4937883_a32eb44246273a47eccac3665e2fddba.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4931742_3dc11fb3e8b1f91d16a891026a1b7075.jp…/content/kohya-trainer/train_data/danbooru_4931742_3dc11fb3e8b1f91d16a891026a1b7075.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4923462_4a41aaea23f2759965968c57df88eb1c.jp…/content/kohya-trainer/train_data/danbooru_4923462_4a41aaea23f2759965968c57df88eb1c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4919569_e96142d96f6c2f3e9c693489fb8ffd97.jp…/content/kohya-trainer/train_data/danbooru_4919569_e96142d96f6c2f3e9c693489fb8ffd97.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4846230_59bf7e6ded93a222e912a25ec841c065.jp…/content/kohya-trainer/train_data/danbooru_4846230_59bf7e6ded93a222e912a25ec841c065.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4823156_eeabf8c10e06e854821479c7dd206a5b.jp…/content/kohya-trainer/train_data/danbooru_4823156_eeabf8c10e06e854821479c7dd206a5b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4819285_4334999e93c1d4b209ba8dda530e2f85.jp…/content/kohya-trainer/train_data/danbooru_4819285_4334999e93c1d4b209ba8dda530e2f85.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4806160_f1eeef8b4a43adf3361eba605ea50c4c.jp…/content/kohya-trainer/train_data/danbooru_4806160_f1eeef8b4a43adf3361eba605ea50c4c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4806158_f42e9ddcf3cfc46bbd871959738463bf.jp…/content/kohya-trainer/train_data/danbooru_4806158_f42e9ddcf3cfc46bbd871959738463bf.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4806154_75263689997c41fd1f2d7f8747fb5b2c.jp…/content/kohya-trainer/train_data/danbooru_4806154_75263689997c41fd1f2d7f8747fb5b2c.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4805412_c5de34fcff1df56bc1fb4f84250947fc.jp…/content/kohya-trainer/train_data/danbooru_4805412_c5de34fcff1df56bc1fb4f84250947fc.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4804212_ae57d1e2716de8ddbc8006cb290a00b3.jp…/content/kohya-trainer/train_data/danbooru_4804212_ae57d1e2716de8ddbc8006cb290a00b3.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4781531_27c26181bd95fc3e0e3d75c2128635b4.jp…/content/kohya-trainer/train_data/danbooru_4781531_27c26181bd95fc3e0e3d75c2128635b4.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4769695_ce89a10607fdbdf46d688a597ea9f902.jp…/content/kohya-trainer/train_data/danbooru_4769695_ce89a10607fdbdf46d688a597ea9f902.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4758360_9d3f64500773358ca962bba4f0a5c20e.jp…/content/kohya-trainer/train_data/danbooru_4758360_9d3f64500773358ca962bba4f0a5c20e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4700833_0a7459e09a387cc0feedc262a9c04d07.jp…/content/kohya-trainer/train_data/danbooru_4700833_0a7459e09a387cc0feedc262a9c04d07.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4688749_9c6ae19b40269511d31959c535342800.jp…/content/kohya-trainer/train_data/danbooru_4688749_9c6ae19b40269511d31959c535342800.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4667640_3efcd08f22a264ba0a71315360800532.jp…/content/kohya-trainer/train_data/danbooru_4667640_3efcd08f22a264ba0a71315360800532.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4657755_60230844f4a1320a35e05bee8fb58555.jp…/content/kohya-trainer/train_data/danbooru_4657755_60230844f4a1320a35e05bee8fb58555.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4635096_7dad3f19f4c48a0d6686a140bbf3d434.jp…/content/kohya-trainer/train_data/danbooru_4635096_7dad3f19f4c48a0d6686a140bbf3d434.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4625260_dff628210c897822de7b71357b867f1d.jp…/content/kohya-trainer/train_data/danbooru_4625260_dff628210c897822de7b71357b867f1d.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4621751_b49377cfe364d35c382a36a8126ac379.jp…/content/kohya-trainer/train_data/danbooru_4621751_b49377cfe364d35c382a36a8126ac379.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4613931_d26f773d357c8945dd82534d9391f8be.jp…/content/kohya-trainer/train_data/danbooru_4613931_d26f773d357c8945dd82534d9391f8be.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4609483_21b7abfdb5cf0de0d0655302cef21d3e.jp…/content/kohya-trainer/train_data/danbooru_4609483_21b7abfdb5cf0de0d0655302cef21d3e.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4608810_bb5fc1518a75b64419f3b71d0b5a7db2.jp…/content/kohya-trainer/train_data/danbooru_4608810_bb5fc1518a75b64419f3b71d0b5a7db2.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4601197_e9afd3b3574a611c85649cddb8ee92d7.jp…/content/kohya-trainer/train_data/danbooru_4601197_e9afd3b3574a611c85649cddb8ee92d7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4572980_4eeb48efdeafe1e9d316000109fe83ac.jp…/content/kohya-trainer/train_data/danbooru_4572980_4eeb48efdeafe1e9d316000109fe83ac.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4571009_a6e926c4142ec0dfb28ad225b2361c2b.jp…/content/kohya-trainer/train_data/danbooru_4571009_a6e926c4142ec0dfb28ad225b2361c2b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4554989_3cf2dea277e209c4a0dfc2a4705b3efe.jp…/content/kohya-trainer/train_data/danbooru_4554989_3cf2dea277e209c4a0dfc2a4705b3efe.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4546950_34044b761c3496d700a6b49e49adafe7.jp…/content/kohya-trainer/train_data/danbooru_4546950_34044b761c3496d700a6b49e49adafe7.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4545531_7df4aaeca430160596c5e6a7e28b018b.jp…/content/kohya-trainer/train_data/danbooru_4545531_7df4aaeca430160596c5e6a7e28b018b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4539294_5c6092b95c75e4e8d47ed821e439714b.jp…/content/kohya-trainer/train_data/danbooru_4539294_5c6092b95c75e4e8d47ed821e439714b.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4535774_46437f1129b68174775d40e588b10060.jp…/content/kohya-trainer/train_data/danbooru_4535774_46437f1129b68174775d40e588b10060.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4527534_775f82e55d8b883c6e1e711a0ac0b053.jp…/content/kohya-trainer/train_data/danbooru_4527534_775f82e55d8b883c6e1e711a0ac0b053.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4488816_126b7cafe0c4a2c0549eb42764b28282.jp…/content/kohya-trainer/train_data/danbooru_4488816_126b7cafe0c4a2c0549eb42764b28282.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4487951_e4e63c0b0dd0083871e0a8bec2392c76.jp…/content/kohya-trainer/train_data/danbooru_4487951_e4e63c0b0dd0083871e0a8bec2392c76.jpg\u001b[0m\n",
            "\u001b[1;32m/content/kohya-trainer/train_data/danbooru_4327801_23694e72747e820f593e2fbc93d0319e.jp…/content/kohya-trainer/train_data/danbooru_4327801_23694e72747e820f593e2fbc93d0319e.jpg\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`(NEW)` Waifu Diffusion 1.4 Autotagger"
      ],
      "metadata": {
        "id": "SoPUJaTpTusz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Tensorflow\n",
        "%cd /content/\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "cellView": "form",
        "id": "POJhWn28XrPs",
        "outputId": "a62695e0-4757-4f4e-d0bb-d7311911cfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Weight\n",
        "%cd /content/kohya-trainer/\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def huggingface_dl(url, weight):\n",
        "  user_token = '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'\n",
        "  user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "  !wget -c --header={user_header} {url} -O /content/kohya-trainer/wd14tagger-weight/{weight}\n",
        "\n",
        "def download_weight():\n",
        "  !mkdir /content/kohya-trainer/wd14tagger-weight/\n",
        "  huggingface_dl(\"https://huggingface.co/Linaqruf/personal_backup/resolve/main/wd14tagger-weight/wd14Tagger.zip\", \"wd14Tagger.zip\")\n",
        "  \n",
        "  !unzip /content/kohya-trainer/wd14tagger-weight/wd14Tagger.zip -d /content/kohya-trainer/wd14tagger-weight\n",
        "\n",
        "  # Destination path \n",
        "  destination = '/content/kohya-trainer/wd14tagger-weight'\n",
        "\n",
        "  if os.path.isfile('/content/kohya-trainer/tag_images_by_wd14_tagger.py'):\n",
        "    # Move the content of \n",
        "    # source to destination \n",
        "    shutil.move(\"tag_images_by_wd14_tagger.py\", destination) \n",
        "  else:\n",
        "    pass\n",
        "\n",
        "download_weight()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WDSlAEHzT2Im",
        "outputId": "7b0603b7-01cd-4c1a-972f-59ef69d264d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "--2022-11-25 13:29:51--  https://huggingface.co/Linaqruf/personal_backup/resolve/main/wd14tagger-weight/wd14Tagger.zip\n",
            "Resolving huggingface.co (huggingface.co)... 44.196.197.193, 54.147.99.175, 2600:1f18:147f:e800:3df1:c2fc:20aa:9b45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|44.196.197.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/c5/e0/c5e0db63eaad7fb0ffbad7a63d59aeb7caf05c7747d0f4bbd9f487c4cf7d9145/b92b8e96687c354029a260a3b9e68d44390a758da1fca06028e312d686acaec3?response-content-disposition=attachment%3B%20filename%3D%22wd14Tagger.zip%22&Expires=1669642192&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2M1L2UwL2M1ZTBkYjYzZWFhZDdmYjBmZmJhZDdhNjNkNTlhZWI3Y2FmMDVjNzc0N2QwZjRiYmQ5ZjQ4N2M0Y2Y3ZDkxNDUvYjkyYjhlOTY2ODdjMzU0MDI5YTI2MGEzYjllNjhkNDQzOTBhNzU4ZGExZmNhMDYwMjhlMzEyZDY4NmFjYWVjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMndkMTRUYWdnZXIuemlwJTIyIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjY5NjQyMTkyfX19XX0_&Signature=gtpm3OVUS6XEX6O0yN~cUBRIw2yfJnhAm9W0o2cG~nzlgkgWeCDISzklnZ-i1FYOV1PUPD2vaYUZc46GSW0Y1Uo4qjjnO7QnP-HbNxIjFkxB4oVm45aqcOsMps67FP3YobSiAIxj2jp8pyoH2cWBbgl6Uw0nIsFJrJict9xqq8Joe8y5iSw~IIxTMtgCvy2me8xFnsFmXdqynk0GcuyymsYFI0vNaHrxffkMtBi-FlnihyQxO3Lz0~f703yqZnfiFfJTWEz6kM7vlIJqmcpJpo7uUlANqgKWXtnBciw5KAP8k4N3iPv7w-GEMRNffcXLsNubZXE1GUvKocMzE0jMUw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-11-25 13:29:52--  https://cdn-lfs.huggingface.co/repos/c5/e0/c5e0db63eaad7fb0ffbad7a63d59aeb7caf05c7747d0f4bbd9f487c4cf7d9145/b92b8e96687c354029a260a3b9e68d44390a758da1fca06028e312d686acaec3?response-content-disposition=attachment%3B%20filename%3D%22wd14Tagger.zip%22&Expires=1669642192&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2M1L2UwL2M1ZTBkYjYzZWFhZDdmYjBmZmJhZDdhNjNkNTlhZWI3Y2FmMDVjNzc0N2QwZjRiYmQ5ZjQ4N2M0Y2Y3ZDkxNDUvYjkyYjhlOTY2ODdjMzU0MDI5YTI2MGEzYjllNjhkNDQzOTBhNzU4ZGExZmNhMDYwMjhlMzEyZDY4NmFjYWVjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMndkMTRUYWdnZXIuemlwJTIyIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjY5NjQyMTkyfX19XX0_&Signature=gtpm3OVUS6XEX6O0yN~cUBRIw2yfJnhAm9W0o2cG~nzlgkgWeCDISzklnZ-i1FYOV1PUPD2vaYUZc46GSW0Y1Uo4qjjnO7QnP-HbNxIjFkxB4oVm45aqcOsMps67FP3YobSiAIxj2jp8pyoH2cWBbgl6Uw0nIsFJrJict9xqq8Joe8y5iSw~IIxTMtgCvy2me8xFnsFmXdqynk0GcuyymsYFI0vNaHrxffkMtBi-FlnihyQxO3Lz0~f703yqZnfiFfJTWEz6kM7vlIJqmcpJpo7uUlANqgKWXtnBciw5KAP8k4N3iPv7w-GEMRNffcXLsNubZXE1GUvKocMzE0jMUw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.94, 18.155.68.98, 18.155.68.73, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 339807043 (324M) [application/octet-stream]\n",
            "Saving to: ‘/content/kohya-trainer/wd14tagger-weight/wd14Tagger.zip’\n",
            "\n",
            "/content/kohya-trai 100%[===================>] 324.06M   297MB/s    in 1.1s    \n",
            "\n",
            "2022-11-25 13:29:53 (297 MB/s) - ‘/content/kohya-trainer/wd14tagger-weight/wd14Tagger.zip’ saved [339807043/339807043]\n",
            "\n",
            "Archive:  /content/kohya-trainer/wd14tagger-weight/wd14Tagger.zip\n",
            "   creating: /content/kohya-trainer/wd14tagger-weight/networks/\n",
            "   creating: /content/kohya-trainer/wd14tagger-weight/networks/ViTB16_11_03_2022_07h05m53s/\n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/networks/ViTB16_11_03_2022_07h05m53s/keras_metadata.pb  \n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/networks/ViTB16_11_03_2022_07h05m53s/saved_model.pb  \n",
            "   creating: /content/kohya-trainer/wd14tagger-weight/networks/ViTB16_11_03_2022_07h05m53s/variables/\n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/networks/ViTB16_11_03_2022_07h05m53s/variables/variables.data-00000-of-00001  \n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/networks/ViTB16_11_03_2022_07h05m53s/variables/variables.index  \n",
            "   creating: /content/kohya-trainer/wd14tagger-weight/2022_0000_0899_6549/\n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/2022_0000_0899_6549/selected_tags.csv  \n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/82148729_p0.jpg  \n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/test_savedmodel.py  \n",
            "   creating: /content/kohya-trainer/wd14tagger-weight/Utils/\n",
            "  inflating: /content/kohya-trainer/wd14tagger-weight/Utils/dbimutils.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Autotagger\n",
        "%cd /content/kohya-trainer/wd14tagger-weight\n",
        "!python tag_images_by_wd14_tagger.py --batch_size 4 /content/kohya-trainer/train_data\n",
        "\n",
        "#@markdown Args list:\n",
        "#@markdown - `--train_data_dir` : directory for training images\n",
        "#@markdown - `--model` : model path to load\n",
        "#@markdown - `--tag_csv` : csv file for tag\n",
        "#@markdown - `--thresh` : threshold of confidence to add a tag\n",
        "#@markdown - `--batch_size` : batch size in inference\n",
        "#@markdown - `--model` : model path to load\n",
        "#@markdown - `--caption_extension` : extension of caption file\n",
        "#@markdown - `--debug` : debug mode\n"
      ],
      "metadata": {
        "id": "hibZK5NPTjZQ",
        "cellView": "form",
        "outputId": "fdbd2885-c00c-4d97-af10-0c45b4aadd49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer/wd14tagger-weight\n",
            "found 191 images.\n",
            "loading model and labels\n",
            "2022-11-25 13:30:04.991869: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "100% 191/191 [01:02<00:00,  3.06it/s]\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Metadata.json\n",
        "%cd /content/kohya-trainer\n",
        "!python merge_dd_tags_to_metadata.py train_data meta_cap_dd.json"
      ],
      "metadata": {
        "id": "hz2Cmlf2ay9w",
        "cellView": "form",
        "outputId": "ae554e37-810b-4074-a1bd-535e0b288145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "found 191 images.\n",
            "new metadata will be created / 新しいメタデータファイルが作成されます\n",
            "merge tags to metadata json.\n",
            "\r  0% 0/191 [00:00<?, ?it/s]\r100% 191/191 [00:00<00:00, 38224.64it/s]\n",
            "writing metadata: meta_cap_dd.json\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Pre-trained Model "
      ],
      "metadata": {
        "id": "3gob9_OwTlwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Pre-trained Model \n",
        "%cd /content/kohya-trainer\n",
        "!mkdir checkpoint\n",
        "\n",
        "#@title Install Pre-trained Model \n",
        "\n",
        "installModels=[]\n",
        "\n",
        "\n",
        "#@markdown ### Available Model\n",
        "#@markdown Select one of available pretrained model to download:\n",
        "modelUrl = [\"\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/model-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/personal_backup/resolve/main/animeckpt/modelsfw-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned-fp16.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned-fp32.ckpt\", \\\n",
        "            \"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned.ckpt\", \\\n",
        "            \"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\" \\\n",
        "            \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\", \\\n",
        "            \"https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\"]\n",
        "modelList = [\"\", \\\n",
        "             \"Animefull-final-pruned\", \\\n",
        "             \"Animesfw-final-pruned\", \\\n",
        "             \"Anything-V3.0-pruned-fp16\", \\\n",
        "             \"Anything-V3.0-pruned-fp32\", \\\n",
        "             \"Anything-V3.0-pruned\", \\\n",
        "             \"Stable-Diffusion-v1-4\", \\\n",
        "             \"Stable-Diffusion-v1-5-pruned-emaonly\" \\\n",
        "             \"Waifu-Diffusion-v1-3-fp32\"]\n",
        "modelName = \"Anything-V3.0-pruned-fp32\" #@param [\"\", \"Animefull-final-pruned\", \"Animesfw-final-pruned\", \"Anything-V3.0-pruned-fp16\", \"Anything-V3.0-pruned-fp32\", \"Anything-V3.0-pruned\", \"Stable-Diffusion-v1-4\", \"Stable-Diffusion-v1-5-pruned-emaonly\", \"Waifu-Diffusion-v1-3-fp32\"]\n",
        "\n",
        "#@markdown ### Custom model\n",
        "#@markdown The model URL should be a direct download link.\n",
        "customName = \"\" #@param {'type': 'string'}\n",
        "customUrl = \"\"#@param {'type': 'string'}\n",
        "\n",
        "if customName == \"\" or customUrl == \"\":\n",
        "  pass\n",
        "else:\n",
        "  installModels.append((customName, customUrl))\n",
        "\n",
        "if modelName != \"\":\n",
        "  # Map model to URL\n",
        "  installModels.append((modelName, modelUrl[modelList.index(modelName)]))\n",
        "\n",
        "def install_aria():\n",
        "  if not os.path.exists('/usr/bin/aria2c'):\n",
        "    !apt install -y -qq aria2\n",
        "\n",
        "def install(checkpoint_name, url):\n",
        "  if url.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy -O \"/content/kohya-trainer/checkpoint/{checkpoint_name}.ckpt\" \"{url}\"\n",
        "  elif url.startswith(\"magnet:?\"):\n",
        "    install_aria()\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 -o /content/kohya-trainer/checkpoint/{checkpoint_name}.ckpt \"{url}\"\n",
        "  else:\n",
        "    user_token = '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'\n",
        "    user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "    !wget -c --header={user_header} \"{url}\" -O /content/kohya-trainer/checkpoint/{checkpoint_name}.ckpt\n",
        "\n",
        "def install_checkpoint():\n",
        "  for model in installModels:\n",
        "    install(model[0], model[1])\n",
        "install_checkpoint()\n",
        "\n"
      ],
      "metadata": {
        "id": "SoucgZQ6jgPQ",
        "cellView": "form",
        "outputId": "e7072bdf-293a-4ebd-f288-881770189c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "--2022-11-25 13:31:22--  https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned-fp32.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 54.147.99.175, 44.196.197.193, 2600:1f18:147f:e800:3df1:c2fc:20aa:9b45, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.147.99.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/1e/ef/1eefd55077badc87cd1798672a058b8d55aeab58e781be883f2ec0e0917679e3/67a115286b56c086b36e323cfef32d7e3afbe20c750c4386a238a11feb6872f7?response-content-disposition=attachment%3B%20filename%3D%22Anything-V3.0-pruned-fp32.ckpt%22&Expires=1669626392&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzFlL2VmLzFlZWZkNTUwNzdiYWRjODdjZDE3OTg2NzJhMDU4YjhkNTVhZWFiNThlNzgxYmU4ODNmMmVjMGUwOTE3Njc5ZTMvNjdhMTE1Mjg2YjU2YzA4NmIzNmUzMjNjZmVmMzJkN2UzYWZiZTIwYzc1MGM0Mzg2YTIzOGExMWZlYjY4NzJmNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMkFueXRoaW5nLVYzLjAtcHJ1bmVkLWZwMzIuY2twdCUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY2OTYyNjM5Mn19fV19&Signature=hoQeGcFpjwPY9bnru7~GzyiyNOPrLQqRByyhLDQN888p3RYP8IbGkWOLVkI78ilq-FycFOSkFTT-ghpIgZgGwsFKiOfMN8DelpfMHtR3vGkQ8uhxto-Vxzjx96WLr8q7rxCEDB4JhXhyBf~lLRF3Ay-MPvvj~x6Z2eBSzLVZ4Ybvh2AOSCqY7Eu2cdG8QLnS2srlFcFJ-moMXfgRgP0kpgrYqfdAJBrUKV~01D6cGdCtDBjgg4qUJoJlQliNGiJpnRQmQWoD3KUM5hkBlWMLtYpJKCiPJb68ojf7GoHfsj84VR6Oqw3K732ajm9nZeztDSsrd0pnDhJ1jzxWBzvLhQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-11-25 13:31:23--  https://cdn-lfs.huggingface.co/repos/1e/ef/1eefd55077badc87cd1798672a058b8d55aeab58e781be883f2ec0e0917679e3/67a115286b56c086b36e323cfef32d7e3afbe20c750c4386a238a11feb6872f7?response-content-disposition=attachment%3B%20filename%3D%22Anything-V3.0-pruned-fp32.ckpt%22&Expires=1669626392&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzFlL2VmLzFlZWZkNTUwNzdiYWRjODdjZDE3OTg2NzJhMDU4YjhkNTVhZWFiNThlNzgxYmU4ODNmMmVjMGUwOTE3Njc5ZTMvNjdhMTE1Mjg2YjU2YzA4NmIzNmUzMjNjZmVmMzJkN2UzYWZiZTIwYzc1MGM0Mzg2YTIzOGExMWZlYjY4NzJmNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMkFueXRoaW5nLVYzLjAtcHJ1bmVkLWZwMzIuY2twdCUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY2OTYyNjM5Mn19fV19&Signature=hoQeGcFpjwPY9bnru7~GzyiyNOPrLQqRByyhLDQN888p3RYP8IbGkWOLVkI78ilq-FycFOSkFTT-ghpIgZgGwsFKiOfMN8DelpfMHtR3vGkQ8uhxto-Vxzjx96WLr8q7rxCEDB4JhXhyBf~lLRF3Ay-MPvvj~x6Z2eBSzLVZ4Ybvh2AOSCqY7Eu2cdG8QLnS2srlFcFJ-moMXfgRgP0kpgrYqfdAJBrUKV~01D6cGdCtDBjgg4qUJoJlQliNGiJpnRQmQWoD3KUM5hkBlWMLtYpJKCiPJb68ojf7GoHfsj84VR6Oqw3K732ajm9nZeztDSsrd0pnDhJ1jzxWBzvLhQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.128, 18.155.68.94, 18.155.68.98, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4265327916 (4.0G) [application/octet-stream]\n",
            "Saving to: ‘/content/kohya-trainer/checkpoint/Anything-V3.0-pruned-fp32.ckpt’\n",
            "\n",
            "/content/kohya-trai 100%[===================>]   3.97G  60.0MB/s    in 50s     \n",
            "\n",
            "2022-11-25 13:32:13 (81.2 MB/s) - ‘/content/kohya-trainer/checkpoint/Anything-V3.0-pruned-fp32.ckpt’ saved [4265327916/4265327916]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Training"
      ],
      "metadata": {
        "id": "15xUbLvQNN28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aspect Ratio Bucketing\n",
        "%cd /content/kohya-trainer\n",
        "\n",
        "model_dir = \"/content/kohya-trainer/checkpoint/Anything-V3.0-pruned-fp32.ckpt\" #@param {'type' : 'string'} \n",
        "batch_size = 4 #@param {'type':'integer'}\n",
        "max_resolution = \"512,512\" #@param [\"512,512\", \"768,768\"] {allow-input: false}\n",
        "mixed_precision = \"no\" #@param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "!python prepare_buckets_latents.py train_data meta_cap_dd.json meta_lat.json {model_dir} \\\n",
        "  --batch_size {batch_size} \\\n",
        "  --max_resolution {max_resolution} \\\n",
        "  --mixed_precision {mixed_precision}"
      ],
      "metadata": {
        "id": "hhgatqF3leHJ",
        "cellView": "form",
        "outputId": "b597b2d0-5b51-46eb-daf5-904ad6e5eba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "found 191 images.\n",
            "loading existing metadata: meta_cap_dd.json\n",
            "load StableDiffusion checkpoint\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'logit_scale', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'text_projection.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100% 191/191 [01:31<00:00,  2.08it/s]\n",
            "bucket 0 (256, 832): 0\n",
            "bucket 1 (256, 896): 0\n",
            "bucket 2 (256, 960): 0\n",
            "bucket 3 (256, 1024): 0\n",
            "bucket 4 (320, 704): 11\n",
            "bucket 5 (320, 768): 0\n",
            "bucket 6 (384, 640): 82\n",
            "bucket 7 (448, 576): 97\n",
            "bucket 8 (512, 512): 0\n",
            "bucket 9 (576, 448): 1\n",
            "bucket 10 (640, 384): 0\n",
            "bucket 11 (704, 320): 0\n",
            "bucket 12 (768, 320): 0\n",
            "bucket 13 (832, 256): 0\n",
            "bucket 14 (896, 256): 0\n",
            "bucket 15 (960, 256): 0\n",
            "bucket 16 (1024, 256): 0\n",
            "mean ar error: 0.051413541645954824\n",
            "writing metadata: meta_lat.json\n",
            "done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set config for `!Accelerate`\n",
        "#@markdown #Hint\n",
        "\n",
        "#@markdown 1. **In which compute environment are you running?** ([0] This machine, [1] AWS (Amazon SageMaker)): `0`\n",
        "#@markdown 2. **Which type of machine are you using?** ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU [4] MPS): `0`\n",
        "#@markdown 3. **Do you want to run your training on CPU only (even if a GPU is available)?** [yes/NO]: `NO`\n",
        "#@markdown 4. **Do you want to use DeepSpeed?** [yes/NO]: `NO`\n",
        "#@markdown 5. **What GPU(s) (by id) should be used for training on this machine as a comma-seperated list?** [all] = `all`\n",
        "#@markdown 6. **Do you wish to use FP16 or BF16 (mixed precision)?** [NO/fp16/bf16]: `fp16`\n",
        "!accelerate config"
      ],
      "metadata": {
        "id": "RnjHb4wgD7vu",
        "outputId": "703eb57e-750e-40ca-9849-58fee308b19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU [4] MPS): 0\n",
            "Do you want to run your training on CPU only (even if a GPU is available)? [yes/NO]:no\n",
            "Do you want to use DeepSpeed? [yes/NO]: no\n",
            "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:all\n",
            "Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: fp16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training\n",
        "\n"
      ],
      "metadata": {
        "id": "yHNbl3O_NSS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training begin\n",
        "num_cpu_threads_per_process = 8 #@param {'type':'integer'}\n",
        "model_path =\"/content/kohya-trainer/checkpoint/Anything-V3.0-pruned-fp32.ckpt\" #@param {'type':'string'}\n",
        "output_dir =\"/content/kohya-trainer/fine_tuned\" #@param {'type':'string'}\n",
        "train_batch_size = 1  #@param {type: \"slider\", min: 1, max: 10}\n",
        "learning_rate =\"2e-6\" #@param {'type':'string'}\n",
        "max_token_length = \"225\" #@param  [\"150\", \"225\"] {allow-input: false}\n",
        "clip_skip = 2 #@param {type: \"slider\", min: 1, max: 10}\n",
        "mixed_precision = \"fp16\" #@param [\"fp16\", \"bf16\"] {allow-input: false}\n",
        "max_train_steps = 5000 #@param {'type':'integer'}\n",
        "# save_precision = \"fp16\" #@param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_every_n_epochs = 10 #@param {'type':'integer'}\n",
        "gradient_accumulation_steps = 1 #@param {type: \"slider\", min: 1, max: 10}\n",
        "\n",
        "  \n",
        "%cd /content/kohya-trainer\n",
        "!accelerate launch --num_cpu_threads_per_process {num_cpu_threads_per_process} fine_tune.py \\\n",
        "  --pretrained_model_name_or_path={model_path} \\\n",
        "  --in_json meta_lat.json \\\n",
        "  --train_data_dir=train_data \\\n",
        "  --output_dir={output_dir} \\\n",
        "  --shuffle_caption \\\n",
        "  --train_batch_size={train_batch_size} \\\n",
        "  --learning_rate={learning_rate} \\\n",
        "  --max_token_length={max_token_length} \\\n",
        "  --clip_skip={clip_skip} \\\n",
        "  --mixed_precision={mixed_precision} \\\n",
        "  --max_train_steps={max_train_steps}  \\\n",
        "  --use_8bit_adam \\\n",
        "  --xformers \\\n",
        "  --gradient_checkpointing \\\n",
        "  --save_every_n_epochs={save_every_n_epochs} \\\n",
        "  --save_state \\\n",
        "  --gradient_accumulation_steps {gradient_accumulation_steps}\n",
        "  # --save_precision={save_precision} \n",
        "  # --resume /content/kohya-trainer/checkpoint/last-state\n"
      ],
      "metadata": {
        "id": "X_Rd3Eh07xlA",
        "cellView": "form",
        "outputId": "1723f0b0-fb25-4874-bb7c-6fa9ee4e0848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n",
            "loading existing metadata: meta_lat.json\n",
            "prepare tokenizer\n",
            "update token length in tokenizer: 225\n",
            "prepare dataset\n",
            "make buckets\n",
            "number of buckets: 4\n",
            "prepare accelerator\n",
            "load StableDiffusion checkpoint\n",
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'text_projection.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Replace CrossAttention.forward to use xformers\n",
            "prepare optimizer, data loader etc.\n",
            "use 8-bit Adam optimizer\n",
            "running training / 学習開始\n",
            "  num examples / サンプル数: 191\n",
            "  num batches per epoch / 1epochのバッチ数: 191\n",
            "  num epochs / epoch数: 27\n",
            "  batch size per device / バッチサイズ: 1\n",
            "  total train batch size (with parallel & distributed) / 総バッチサイズ（並列学習含む）: 1\n",
            "  gradient ccumulation steps / 勾配を合計するステップ数 = 1\n",
            "  total optimization steps / 学習ステップ数: 5000\n",
            "steps:   0% 0/5000 [00:00<?, ?it/s]epoch 1/27\n",
            "steps:   4% 191/5000 [02:50<1:11:05,  1.13it/s, loss=0.0978]epoch 2/27\n",
            "steps:   7% 371/5000 [05:23<1:05:27,  1.18it/s, loss=0.0967]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Miscellaneous"
      ],
      "metadata": {
        "id": "vqfgyL-thgdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model Pruner\n",
        "#@markdown Do you want to Pruning model?\n",
        "\n",
        "prune = False #@param {'type':'boolean'}\n",
        "\n",
        "model_path = \"/content/kohya-trainer/fine_tuned/last.ckpt\" #@param {'type' : 'string'}\n",
        "if prune == True:\n",
        "  import os\n",
        "  if os.path.isfile('/content/prune-ckpt.py'):\n",
        "    pass\n",
        "  else:\n",
        "    !wget https://raw.githubusercontent.com/prettydeep/Dreambooth-SD-ckpt-pruning/main/prune-ckpt.py\n",
        "\n",
        "\n",
        "  !python prune-ckpt.py --ckpt {model_path}\n",
        "\n"
      ],
      "metadata": {
        "id": "LUOG7BzQVLKp",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount to Google Drive\n",
        "mount_drive= False #@param {'type':'boolean'}\n",
        "\n",
        "if mount_drive== True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OuRqOSp2eU6t",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Huggingface_hub Integration"
      ],
      "metadata": {
        "id": "QtVP2le8PL2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Instruction:\n",
        "0. Of course you need a Huggingface Account first\n",
        "1. Create huggingface token, go to `Profile > Access Tokens > New Token > Create a new access token` with the `Write` role.\n",
        "2. All cells below are checked `opt-out` by default so you need to uncheck it if you want to running the cells."
      ],
      "metadata": {
        "id": "tbKgmh_AO5NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to Huggingface hub\n",
        "#@markdown Opt-out this cell when run all\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "\n",
        "#@markdown Prepare your Huggingface token\n",
        "\n",
        "saved_token= \"save-your-write-token-here\" #@param {'type': 'string'}\n",
        "\n",
        "if opt_out == False:\n",
        "  from huggingface_hub import notebook_login\n",
        "  notebook_login()\n",
        "\n"
      ],
      "metadata": {
        "id": "Da7awoqAPJ3a",
        "outputId": "514312a7-a254-4db6-cdf2-6be398e71dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>This cell will not running because you choose to opt-out this cell.<h1>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Commit trained model to Huggingface"
      ],
      "metadata": {
        "id": "jypUkLWc48R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instruction:\n",
        "0. Create huggingface repository for model\n",
        "1. Clone your model to this colab session\n",
        "2. Move these necessary file to your repository to save your trained model to huggingface\n",
        "\n",
        ">in `content/kohya-trainer/fine-tuned`\n",
        "- File `epoch-nnnnn.ckpt` and/or\n",
        "- File `last.ckpt`, \n",
        "\n",
        "4. Commit your model to huggingface"
      ],
      "metadata": {
        "id": "TvZgRSmKVSRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone Model\n",
        "\n",
        "#@markdown Opt-out this cell when run all\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "\n",
        "if opt_out == False:\n",
        "  %cd /content\n",
        "  Repository_url = \"https://huggingface.co/Linaqruf/alphanime-diffusion\" #@param {'type': 'string'}\n",
        "  !git clone {Repository_url}\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "182Law9oUiYN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title `NEW!` Move trained model to cloned repository\n",
        "#@markdown Opt-out this cell when run all\n",
        "\n",
        "import shutil\n",
        "\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "#@markdown Fill necessary file/folder path in the textbox given below. You need to atleast already cloned models and/or datasets from huggingface.\n",
        "\n",
        "model_path = \"/content/granblue-fantasy\" #@param {'type' : 'string'}\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "#model\n",
        "last_pruned_ckpt = \"/content/kohya-trainer/fine_tuned/last-pruned.ckpt\" #@param {'type' : 'string'}\n",
        "last_ckpt = \"/content/kohya-trainer/fine_tuned/last.ckpt\" #@param {'type' : 'string'}\n",
        "\n",
        "if opt_out== False:\n",
        "  if os.path.isfile(last_pruned_ckpt):\n",
        "    shutil.move(last_pruned_ckpt,model_path)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if os.path.isfile(last_ckpt):\n",
        "    shutil.move(last_ckpt,model_path)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iiZ95ASZWn0f",
        "outputId": "990a2104-6736-4d73-bfab-03849ec0a92c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Commit to Huggingface\n",
        "#@markdown Opt-out this cell when run all\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "\n",
        "if opt_out == False:\n",
        "  %cd /content\n",
        "  #@markdown Go to your model path\n",
        "  model_path= \"alphanime-diffusion\" #@param {'type': 'string'}\n",
        "\n",
        "  #@markdown Your path look like /content/**model_path**\n",
        "  #@markdown ___\n",
        "  #@markdown #Git Commit\n",
        "\n",
        "  #@markdown Set **git commit identity**\n",
        "\n",
        "  email= \"your-email\" #@param {'type': 'string'}\n",
        "  name= \"your-name\" #@param {'type': 'string'}\n",
        "  #@markdown Set **commit message**\n",
        "  commit_m= \"this is commit message\" #@param {'type': 'string'}\n",
        "\n",
        "  %cd \"/content/{model_path}\"\n",
        "  !git lfs install\n",
        "  !huggingface-cli lfs-enable-largefiles .\n",
        "  !git add .\n",
        "  !git lfs help smudge\n",
        "  !git config --global user.email \"{email}\"\n",
        "  !git config --global user.name \"{name}\"\n",
        "  !git commit -m \"{commit_m}\"\n",
        "  !git push\n",
        "\n",
        "else:\n",
        "  pass"
      ],
      "metadata": {
        "id": "87wG7QIZbtZE",
        "cellView": "form",
        "outputId": "c37a6bc5-faa7-42e6-8ef8-897332052781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>This cell will not running because you choose to opt-out this cell.<h1>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Commit dataset to huggingface"
      ],
      "metadata": {
        "id": "olP2yaK3OKcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Instruction:\n",
        "0. Create huggingface repository for datasets\n",
        "1. Clone your datasets to this colab session\n",
        "2. Move these necessary file to your repository so that you can do resume training next time without rebuild your dataset with this notebook\n",
        "\n",
        ">in `content/kohya-trainer`\n",
        "- Folder `train_data`\n",
        "- File `meta_cap_dd.json`\n",
        "- File `meta_lat.json`\n",
        "\n",
        ">in `content/kohya-trainer/fine-tuned`\n",
        "- Folder `last-state`\n",
        "\n",
        "4. Commit your datasets to huggingface"
      ],
      "metadata": {
        "id": "jiSb0z2CVtc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone Dataset\n",
        "#@markdown Opt-out this cell when run all\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "\n",
        "if opt_out == False:\n",
        "  %cd /content\n",
        "  Repository_url = \"https://huggingface.co/datasets/Linaqruf/alphanime-diffusion-tag\" #@param {'type': 'string'}\n",
        "  !git clone {Repository_url}\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "QhL6UgqDOURK",
        "cellView": "form",
        "outputId": "76e4838b-3432-4cb8-e478-baf563ac5952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>This cell will not running because you choose to opt-out this cell.<h1>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title `NEW!` Move datasets to cloned repository\n",
        "#@markdown Opt-out this cell when run all\n",
        "\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "#@markdown Fill necessary file/folder path in the textbox given below. You need to atleast already cloned models and/or datasets from huggingface.\n",
        "import shutil\n",
        "\n",
        "datasets_path = \"/content/granblue-fantasy-tag\" #@param {'type' : 'string'}\n",
        "\n",
        "#datasets\n",
        "save_state_dir = \"/content/kohya-trainer/fine_tuned/last-state\" #@param {'type' : 'string'}\n",
        "\n",
        "%cd /content/kohya-trainer\n",
        "\n",
        "train_data = \"/content/drive/kohya-trainer/train_data\"\n",
        "meta_cap_dd = \"/content/kohya-trainer/meta_cap_dd.json\"\n",
        "meta_lat = \"/content/kohya-trainer/meta_lat.json\"\n",
        "\n",
        "if opt_out == False:\n",
        "  if os.path.isdir(save_state_dir):\n",
        "    shutil.move(save_state_dir,datasets_path)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if os.path.isdir(train_data):\n",
        "    shutil.move(train_data,datasets_path)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if os.path.isdir(meta_cap_dd):\n",
        "    shutil.move(meta_cap_dd,datasets_path)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  if os.path.isfile(meta_lat):\n",
        "    shutil.move(meta_lat,datasets_path)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "else:\n",
        "  pass\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nkz2HoRYW3Ao",
        "outputId": "46172a48-e0ce-403f-bf87-999382381c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/kohya-trainer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Commit to Huggingface\n",
        "#@markdown Opt-out this cell when run all\n",
        "\n",
        "opt_out= True #@param {'type':'boolean'}\n",
        "\n",
        "if opt_out == False:\n",
        "  %cd /content\n",
        "  #@markdown Go to your model path\n",
        "  dataset_path= \"alphanime-diffusion-tag\" #@param {'type': 'string'}\n",
        "\n",
        "  #@markdown Your path look like /content/**dataset_path**\n",
        "  #@markdown ___\n",
        "  #@markdown #Git Commit\n",
        "\n",
        "  #@markdown Set **git commit identity**\n",
        "\n",
        "  email= \"your-email\" #@param {'type': 'string'}\n",
        "  name= \"your-name\" #@param {'type': 'string'}\n",
        "  #@markdown Set **commit message**\n",
        "  commit_m= \"this is commit message\" #@param {'type': 'string'}\n",
        "\n",
        "  %cd \"/content/{dataset_path}\"\n",
        "  !git lfs install\n",
        "  !huggingface-cli lfs-enable-largefiles .\n",
        "  !git add .\n",
        "  !git lfs help smudge\n",
        "  !git config --global user.email \"{email}\"\n",
        "  !git config --global user.name \"{name}\"\n",
        "  !git commit -m \"{commit_m}\"\n",
        "  !git push\n",
        "\n",
        "else:\n",
        "  pass"
      ],
      "metadata": {
        "id": "abHLg4I0Os5T",
        "cellView": "form",
        "outputId": "3aa23c29-bd98-496f-b898-cf0f92af0fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[1;32m   2971\u001b[0m         \u001b[0;31m# it in the history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2973\u001b[0;31m             \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2974\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2975\u001b[0m             \u001b[0mpreprocessing_exc_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mtransform_cell\u001b[0;34m(self, raw_cell)\u001b[0m\n\u001b[1;32m   3088\u001b[0m         \"\"\"\n\u001b[1;32m   3089\u001b[0m         \u001b[0;31m# Static input transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3090\u001b[0;31m         \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transformer_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/inputtransformer2.py\u001b[0m in \u001b[0;36mtransform_cell\u001b[0;34m(self, cell)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_token_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/inputtransformer2.py\u001b[0m in \u001b[0;36mdo_token_transforms\u001b[0;34m(self, lines)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_token_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRANSFORM_LOOP_LIMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mchanged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_one_token_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchanged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/inputtransformer2.py\u001b[0m in \u001b[0;36mdo_one_token_transform\u001b[0;34m(self, lines)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_cls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_transformers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_by_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/inputtransformer2.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(cls, tokens_by_line)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_by_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# Last token is NEWLINE; look at last but one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m                 \u001b[0;31m# Find the first token that's not INDENT/DEDENT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}